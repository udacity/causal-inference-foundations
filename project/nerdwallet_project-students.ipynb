{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Project Overview**\n",
    "\n",
    "You're a data scientist at **Nerdwallet,** a mid-sized consumer finance company that aired an advertisement during the **Super Bowl** on **February 10, 2025**. Your goal is to estimate how the ad influenced branded search interest using **causal inference techniques**.\n",
    "\n",
    "Google Trends data is **normalized** from 0â€“100, where **100 represents the peak search interest** during the selected time window. This means the data shows *relative* search volume, not absolute counts.\n",
    "\n",
    "Your goal is to report results using your preferred causal inference model. To do this, \n",
    "you'll first explore various potential models, including:\n",
    "\n",
    "- **Interrupted Time Series (ITS)**\n",
    "- **Difference-in-Differences (DiD)**\n",
    "- **Event Studies**\n",
    "- **Synthetic Control Methods**\n",
    "\n",
    "You'll want to test the assumptions of these models when relevant. To make your decision on the preferred model and results that you ultimately report, you'll want to consider which models make sense under the circumstances and are supported using historical (pre-event) data. This could include: \n",
    "\n",
    "- Running **placebo tests** (to check for false positives)\n",
    "- Performing **sensitivity analysis** (to test robustness to assumptions)\n",
    "- Choosing the most credible model and **justifying your findings to stakeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages as needed for the analysis\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Import data**\n",
    "\n",
    "You can find the exact time series for both NerdWallet and search interest for relevant business-related topics at this link: https://trends.google.com/trends/explore?date=2024-11-01%202025-02-28&geo=US&q=%2Fm%2F0hzpb7j,credit%20card%20points,travel%20credit%20card,best%20savings%20account&hl=en\n",
    "\n",
    "NerdWallet is the time series of direct search interest for the company. Search interest for 'credit card points', 'travel credit card', and 'best savings account' help identify the seasonality of interest in companies like NerdWallet. These help control for the seasonality of which users might search for NerdWallet in the absence of the Super Bowl ad. \n",
    "\n",
    "Note that when downloading the file, there are some lines of extra space at the top. You can remove these within pd.read_csv(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code and initial parameters to consider\n",
    "\n",
    "event_date = pd.to_datetime('2025-02-10') # Super Bowl date is '2025-02-10' so if prior then this is a placebo test\n",
    "preperiod_length = 90\n",
    "postperiod_length = 3\n",
    "\n",
    "# Define the start and end dates for the pre and post event periods: Use these to subset your data prior to estimating the models \n",
    "start_date = event_date - pd.DateOffset(days=preperiod_length)\n",
    "end_date = event_date + pd.DateOffset(days=postperiod_length)\n",
    "\n",
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Plot the time series of each keyword**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Start with interrupted time series (ITS).** \n",
    "\n",
    "You only need the columns for date and nerdwallet search interest.  Consider that you make choices for the size of both the pre-treatment and post-treatment window. Plot the model predictions against actual values. How are the in-sample and out-of-sample fit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Next attempt Difference-in-Differences** \n",
    "\n",
    "It is not necessary for difference-in-differences to have just a single untreated comparison group. You will want to first melt the data so that there is one row per search term-x-date.  You should have columns for the date, search term, and the value. \n",
    "\n",
    "Make sure to test the critical assumptions for the DiD model. Again, you may want to consider the pre- and post-period windows and the in-sample and out-of-sample fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Difference-in-Differences with controls** \n",
    "\n",
    "You can sometimes improve precision by including controls. One common approach is to include two-way fixed effects (TWFE). This includes fixed effects for both the date and search term. Make sure though you either exclude one of the categories as the baseline or remove the intercept.  Within smf.ols, you can remove the intercept by adding a '-1' inside the formula. \n",
    "\n",
    "How is the fit? Note if you model with date fixed effects, you will only be able to make in-sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Next attempt the Event Study** \n",
    "\n",
    "Unlike Difference-in-Differences, you do not want one row for each term-x-date in the Event Study. Instead, the data should be structured as it comes out of the Google Trends site with one column for each search term and one column for the dates.\n",
    "\n",
    "First, set up the event study with a single 'post' variable. Then, estimate time-varying effects. Note that you can help validate the model during this step. How long do the effects appear to last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Estimate a synthetic control using Lasso** \n",
    "\n",
    "As in the event study, you'll use data structured so that each search term is a separate column. Both the event study and synthetic control using Lasso estimate regression weights using pre-treatment data. Lasso makes it more likely that some terms will have a zero weight, whereas an exact zero weight is very unlikely in a standard regression model. This could be particularly important in real-world scenarios where you have many possible comparisons in the donor pool.\n",
    "\n",
    "For consistency and simplicity in this first step, use Lasso(alpha=0.1) on pre-treatment data only to obtain the weights. Plot actual vs synthetic control values for the entire period through the end of the data. How is the fit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using synthetic controls is a rapidly evolving field with recent academic papers even in 2025. One quick approach is to treat the synthetic control as your comparison time series and estimate a difference-in-differences model against it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Replicate the synthetic control but optimize model choices using cross validation** \n",
    "\n",
    "We previously used alpha=0.1 as a default. This time, use ML tools such as grid search and cross-validation to help tune and optimize alpha. Then re-estimate the synthetic control weights and DiD as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Use permutation testing for inference for the Synthetic Control** \n",
    "\n",
    "Rather than using difference-in-differences to test how Nerdwallet search trends against the synthetic control, you will replicate the model looping over the donor pool, treating each as if they were treated.  Make sure that nerdwallet--the actual treated term--is NOT included in the donor pool as that will severely distort your results. \n",
    "\n",
    "For each of the potential donors, what is your SC estimate using the same framework as above? Take all those placebo estimates (in this case, there are just three), use the mean and standard deviation and calculate a 95% confidence interval. How does your actual estimate from Nerdwallet compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Which is your preferred model?** \n",
    "\n",
    "Using data prior to the real event data, re-estimate the models and find the placebo effects. In a real-world situation, you would want to clarify with your stakeholders that there were no additional, new marketing events prior to the Super Bowl that could lead to positive 'placebo' effects. \n",
    "\n",
    "**Optional Extra Credit**: While you can get away with model selection using one pre-period date, you can enhance credibility by looping over dates and estimating a distribution of placebo effects. The mean and standard deviation of that distribution can be used for creating empirical confidence intervals. The RMSE of the placebo effects can be used for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Œ **Tell your story** \n",
    "\n",
    "Your stakeholders and concerned about the lack of randomized experimentation and are only vaguely familiar with causal inference methods. Make your pitch! What is the key takeaway? Which model did you choose and why? How confident are you in the results? (\"Confidence\" as used in everyday language, not statistical confidence. This common stakeholder question is often trying to get at: Are you sure the model isn't biased? Is it reasonably precise?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
